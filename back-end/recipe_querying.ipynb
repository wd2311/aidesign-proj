{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 852
    },
    "colab_type": "code",
    "id": "VK-1pKuRQFcu",
    "outputId": "586ca776-391d-4d7e-9602-b2b125d93249"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/hex-3/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/hex-3/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/hex-3/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/hex-3/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/hex-3/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/hex-3/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /home/hex-3/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/hex-3/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/hex-3/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/hex-3/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/hex-3/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/hex-3/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/hex-3/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/hex-3/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /home/hex-3/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/hex-3/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /home/hex-3/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/hex-3/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/hex-3/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20130, 11)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('popular')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "with open('data/full_format_recipes.json') as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "data = pd.DataFrame.from_dict(data, orient='columns')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "t1c7tc0fQOEV",
    "outputId": "869366b1-2422-43e9-f8d0-f1641cbb7cfd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20111, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### drop rows with no ingredients list and no recipes\n",
    "data.dropna(axis = 0, subset = ['ingredients', 'directions'], inplace = True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "997NfRnYQpTM",
    "outputId": "3ce1505d-e646-4cf7-f26c-6dbc6abacef5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16108 4003\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.rand(len(data)) < 0.8\n",
    "\n",
    "train_data = data.iloc[idx]\n",
    "validation_data = data.iloc[~idx]\n",
    "\n",
    "print(len(train_data), len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLrwEZUbQZF1"
   },
   "outputs": [],
   "source": [
    "### common measures used\n",
    "measures= np.array(['liter','millilitres','mL','grams','g', 'kg','teaspoon',\n",
    "          'tsp', 'tablespoon','tbsp','fluid', 'ounce','oz','fl.oz', 'cup','pint','pt',\n",
    "          'quart','qt','gallon','gal','smidgen','drop','pinch','dash','scruple',\n",
    "          'dessertspoon','teacup','cup','c','pottle','gill','dram','wineglass','coffeespoon',\n",
    "          'pound','pounded','lb','tbsp','plus','firmly', 'packed','lightly','level','even',\n",
    "          'rounded','heaping','heaped','sifted','bushel','peck','stick','chopped','sliced',\n",
    "          'halves', 'shredded','slivered','sliced','whole','paste','whole',' fresh', 'peeled', \n",
    "          'diced','mashed','dried','frozen','fresh','peeled','candied','no', 'pulp','crystallized',\n",
    "          'canned','crushed','minced','julienned','clove','head', 'small','large','medium', 'torn', 'cleaned', 'degree'])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "measures = np.array([lemmatizer.lemmatize(m) for m in measures])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "awV6_jP3Qhw4"
   },
   "outputs": [],
   "source": [
    "### make a list of ingredients present in the given recipes\n",
    "\n",
    "meal_ingredients = []\n",
    "\n",
    "for ingredient in train_data['ingredients']:\n",
    "    for ingr in ingredient:\n",
    "      simp_ingr = \"\"\n",
    "      tokens = nltk.word_tokenize(ingr.strip())\n",
    "      tokens = [token for token in tokens if token.isalpha()]\n",
    "      tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "      tokens = [token for token in tokens if token not in measures]\n",
    "      pos_tokens = nltk.pos_tag(tokens)\n",
    "      for tok, tag in pos_tokens:\n",
    "          if tag.startswith(\"N\"):\n",
    "              simp_ingr += tok.lower() + \" \"\n",
    "      simp_ingr = simp_ingr[:-1]\n",
    "      if simp_ingr:\n",
    "        meal_ingredients.append(simp_ingr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "t6K6EjutUUao",
    "outputId": "2dd2079d-6898-47bd-a024-dde7c566102c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stock', 'brown lentil', 'lentil', 'stalk celery', 'carrot', 'sprig thyme', 'kosher salt', 'tomato', 'fuji apple', 'lemon juice', 'oil', 'ground pepper', 'sheet cut crosswise flour tortilla', 'turkey breast', 'bibb lettuce', 'cream', 'onion', 'salt', 'bay leaf', 'garlic', 'pepper', 'ground nutmeg', 'pinch thyme', 'shallot', 'butter', 'boneless center pork sinew cut chunk', 'egg', 'flour', 'tawny port', 'currant', 'lettuce leaf', 'peppercorn', 'minced parsley', 'bay leaf', 'bread baguette slice', 'oil', 'onion', 'currant', 'wine vinegar', 'chicken broth', 'thyme', 'sugar', 'fennel bulb anise stalk bulb cut dice feathery leaf garnish', 'onion', 'butter', 'russet baking potato', 'chicken broth', 'milk', 'oil', 'onion']\n"
     ]
    }
   ],
   "source": [
    "print(meal_ingredients[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1wFkT7fGUbwM",
    "outputId": "d6a2771e-51db-4f64-a751-907927b41707"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29382097, 169443700)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bag of Words Model\n",
    "\n",
    "model = gensim.models.Word2Vec(min_count = 1,  \n",
    "                              size = 100, window = 5) \n",
    "model.build_vocab(meal_ingredients)\n",
    "model.train(meal_ingredients, total_examples = model.corpus_count, \n",
    "            epochs=100, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "CCpIxaVuUilD",
    "outputId": "abd66d65-d7be-4b22-dc93-e62a3eda6e09"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(49, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## vectors for all the words\n",
    "\n",
    "X = model[model.wv.vocab]\n",
    "\n",
    "X.shape\n",
    "\n",
    "### Why does it get so reduced???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "colab_type": "code",
    "id": "c67IUFrwUpKo",
    "outputId": "60bf73fe-f78a-4d04-a3b6-279986a6895c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fa4978d6d68>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlsAAAIICAYAAABZ3ysoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAQB0JGwBAHQkbAEAdPT/AYpSJIv6lByDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##### Visualisation\n",
    "##### The ingredients do not cluster!!\n",
    "\n",
    "cluster_embedding = umap.UMAP(n_neighbors=30, min_dist=0.0,\n",
    "                              n_components=2, random_state=42).fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(10,9))\n",
    "plt.scatter(cluster_embedding[:, 0], cluster_embedding[:, 1], s=3, cmap='Spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "id": "tAAJHe1VU13b",
    "outputId": "24aff426-87a1-43a4-9851-de2742cabc14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0fe8a9378e6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'apple'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m                 )\n\u001b[0;32m-> 1422\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m         \"\"\"\n\u001b[0;32m-> 1397\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.wmdistance() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'apple' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.most_similar(positive=['apple'], topn= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MJjyp-8XV8Pf"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def query(params):\n",
    "\n",
    "    cart = [data[int(idx)] for idx in params['cart']]\n",
    "    allergys = [a for a in params['allergys']]\n",
    "    pantry = [p for p in params['pantry']]\n",
    "\n",
    "    # Candidate Generator\n",
    "    def candidate_generator(num_generated_candidates):\n",
    "        # Vectorize cart: Dictionaries { ingredient : count }\n",
    "        cart_ingredients = np.zeros(len(data[0]['ingredient_vector']))\n",
    "        for recipe in cart:\n",
    "            cart_ingredients = np.maximum(cart_ingredients, recipe['ingredient_vector'])\n",
    "\n",
    "        # Returns true if the recipe does not conflict with the user's allergy restrictions\n",
    "        def allergy_checker(recipe):\n",
    "            allergy_categories = {\n",
    "                'dairy': ['dairy', 'milk', 'cheese', 'butter', 'yogurt', 'cream'],\n",
    "                'nuts': ['nut', 'almond', 'cashew', 'pecan'],\n",
    "                'shellfish': ['shellfish', 'clam', 'oyster', 'mussel', 'scallop'],\n",
    "                'gluten': ['gluten', 'wheat', 'rye', 'barley', 'bread', 'pasta', 'beer', 'bread']\n",
    "            }\n",
    "\n",
    "            # If finds allergy, return false\n",
    "            for allergy in allergys: # for evert allergy,\n",
    "                if allergy in allergy_categories: # if it's in one of our saved allergy categories\n",
    "                    allergy_words = allergy_categories[allergy] # get all the words for that allergy category\n",
    "                    for ingredient in recipe['ingredients']: # for every ingredient\n",
    "                        for allergy_word in allergy_words: # check if any allergy words are in the ingredient\n",
    "                            if allergy_word in ingredient: # if they are, allergy check fails.\n",
    "                                return False\n",
    "            \n",
    "            # If no allergy issue, return true (passed allergy check)\n",
    "            return True   \n",
    "\n",
    "        # Containment of vector a in b\n",
    "        def containment(a, b):\n",
    "            within = np.dot(a, b)\n",
    "            total = np.sum(a)\n",
    "            if total == 0:\n",
    "                return 0\n",
    "            return within / total\n",
    "\n",
    "        # IOU of vectors a and b\n",
    "        def iou(a, b):\n",
    "            intersection = np.sum(np.dot(a, b))\n",
    "            union = np.sum(a) + np.sum(b) - intersection\n",
    "            if union == 0:\n",
    "                return 0\n",
    "            return intersection / union\n",
    "\n",
    "        # Find similar recipes by ingredients, querying our similarity method\n",
    "        candidate_ranks = []\n",
    "        for other_recipe in data:\n",
    "            if allergy_checker(other_recipe): # Only consider a recipe for candidacy if it doesn't violate the user's allergy restrictions\n",
    "                other_ingredients = other_recipe['ingredient_vector']\n",
    "                rank = containment(other_ingredients, cart_ingredients)\n",
    "                candidate_ranks.append((rank, other_recipe))\n",
    "\n",
    "        # Candidates have been generated, by sorting by most similar\n",
    "        generated_candidates = [x[1] for x in sorted(candidate_ranks, reverse=True, key=lambda x: x[0])][:num_generated_candidates]\n",
    "        return generated_candidates\n",
    "\n",
    "    # Ranker\n",
    "    def ranker(generated_candidates, num_ranked_results):\n",
    "        \n",
    "        # Helper method to generate random subset of candidates\n",
    "        def sample_random_subset_of_candidates(generated_candidates, num_ranked_results):\n",
    "            sample_subset = random.sample(generated_candidates, num_ranked_results)\n",
    "            return sample_subset\n",
    "\n",
    "        # Nutrition similarity\n",
    "        def nutrition_similarity(candidate_nutrition, other_candidate_nutrition):\n",
    "            \n",
    "            def iou(a, b):\n",
    "                intersection = np.sum(np.dot(a, b))\n",
    "                union = np.sum(a) + np.sum(b) - intersection\n",
    "                if union == 0:\n",
    "                    return 0\n",
    "                return intersection / union\n",
    "            \n",
    "            nutrition_vector = list(candidate_nutrition.values())\n",
    "            other_nutrition_vector = list(other_candidate_nutrition.values())\n",
    "\n",
    "            similarity = iou(nutrition_vector, other_nutrition_vector)\n",
    "            \n",
    "            return similarity\n",
    "        \n",
    "        # Generate many random subsets of candidates, and measure the nutrition diversity in each\n",
    "        num_sample_subsets = 50\n",
    "        sample_subsets_and_their_nutrition_diversity_scores = []\n",
    "        for i in range(num_sample_subsets):\n",
    "            sample_subset = sample_random_subset_of_candidates(generated_candidates, num_ranked_results)\n",
    "\n",
    "            # For each pair of candidates in the subset, measure the similarity.\n",
    "            subset_nutrition_similarities = []\n",
    "            for candidate in sample_subset:\n",
    "                for otherCandidate in sample_subset:\n",
    "                    if candidate['id'] != otherCandidate['id']:\n",
    "                        # Compare every candidate with every other candidate\n",
    "                        candidate_nutrition = {\n",
    "                            'fat': candidate['fat'],\n",
    "                            'protein': candidate['protein'],\n",
    "                            'sodium': candidate['sodium'],\n",
    "                            'calories': candidate['calories']\n",
    "                        }\n",
    "\n",
    "                        other_candidate_nutrition = {\n",
    "                            'fat': otherCandidate['fat'],\n",
    "                            'protein': otherCandidate['protein'],\n",
    "                            'sodium': otherCandidate['sodium'],\n",
    "                            'calories': otherCandidate['calories']\n",
    "                        }\n",
    "\n",
    "                        # Check for missing nutrient information, don't include in average\n",
    "                        if None not in list(candidate_nutrition.values()) and None not in list(other_candidate_nutrition.values()):\n",
    "                            nutrition_sim = nutrition_similarity(candidate_nutrition, other_candidate_nutrition)\n",
    "                            subset_nutrition_similarities.append(nutrition_sim)\n",
    "\n",
    "            # Average the similarity across pairs of candidates to get a measure of how similar the cluster is to itself\n",
    "            average_inter_subset_nutrition_similarity = sum(subset_nutrition_similarities) / len(subset_nutrition_similarities)\n",
    "\n",
    "            # Diversity score is the opposite of average similarity\n",
    "            nutrition_diversity_score = -1 * average_inter_subset_nutrition_similarity # Diversity is negative similarity\n",
    "\n",
    "            # Record the diversity score for each randomly generated subset\n",
    "            sample_subsets_and_their_nutrition_diversity_scores.append((sample_subset, nutrition_diversity_score))\n",
    "\n",
    "        # Pick the subset with the highest nutrition diversity\n",
    "        most_nutrition_diverse_subset = [subset_score_pair[0] for subset_score_pair in sorted(sample_subsets_and_their_nutrition_diversity_scores, key=lambda x: -x[1], reverse=False)][0]\n",
    "\n",
    "        # Sort resulting items by rating\n",
    "        ranked_results = sorted(most_nutrition_diverse_subset, key=lambda x: -x['rating'])\n",
    "\n",
    "        return ranked_results\n",
    "    \n",
    "    num_generated_candidates = 100\n",
    "    num_ranked_results = 20\n",
    "\n",
    "    generated_candidates = candidate_generator(num_generated_candidates)\n",
    "    final_results = ranker(generated_candidates, num_ranked_results)\n",
    "\n",
    "    return final_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "with open('data/full_format_recipes_plus_normalized_ingredients.json') as f:\n",
    "    data = json.load(f)\n",
    "for i, recipe in enumerate(data):\n",
    "    recipe['id'] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recs(args):  \n",
    "    fields = ['cart', 'allergys', 'pantry']\n",
    "    list_separator = ';;;'\n",
    "    \n",
    "    recommendation_params = {}\n",
    "    \n",
    "    for field in fields:\n",
    "        if field in args:\n",
    "            recommendation_params[field] = args[field].split(list_separator)\n",
    "\n",
    "    recs = query(recommendation_params)\n",
    "\n",
    "    return recs\n",
    "\n",
    "diff_r = []\n",
    "args = [{'cart': '12', 'allergys': 'shellfish', 'pantry': 'carrots'},\n",
    "        {'cart': '10', 'allergys': 'dairy', 'pantry': 'potatoes'},\n",
    "        {'cart': '10', 'allergys': 'gluten', 'pantry': ''},\n",
    "        {'cart': '96', 'allergys': '', 'pantry': 'carrots'},\n",
    "        {'cart': '45', 'allergys': 'nut', 'pantry': ''}]\n",
    "\n",
    "for i in range(5):\n",
    "    diff_r.append(get_recs(args[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.1490712 , 0.28005602, 0.21821789, 0.18257419],\n",
       "       [0.1490712 , 1.        , 0.06262243, 0.09759001, 0.08164966],\n",
       "       [0.28005602, 0.06262243, 1.        , 0.09166985, 0.153393  ],\n",
       "       [0.21821789, 0.09759001, 0.09166985, 1.        , 0.23904572],\n",
       "       [0.18257419, 0.08164966, 0.153393  , 0.23904572, 1.        ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = []\n",
    "for user in diff_r:\n",
    "    x.append(user[0]['ingredient_vector'])\n",
    "x = np.array(x)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sim = cosine_similarity(x)\n",
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.55588997"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([1.        , 0.1490712 , 0.28005602, 0.21821789, 0.18257419, 1.        , 0.06262243, 0.09759001, 0.08164966, 1.        , 0.09166985, 0.153393  , 1.        , 0.23904572, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.55588997"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6.55588997-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.311177994"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.55588997/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "dict_keys(['directions', 'fat', 'date', 'categories', 'calories', 'desc', 'protein', 'rating', 'title', 'ingredients', 'sodium', 'ingredient_names', 'parsed_ingredients', 'ingredient_vector', 'id'])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "diff_r = np.array(diff_r)\n",
    "user = {}\n",
    "recipes_ret= set()\n",
    "for i in range(5):\n",
    "    for rec in (diff_r[i]) : ## iterate over all recipes for one user\n",
    "        print(rec.keys())\n",
    "        #recipes_ret.add(rec['title'])\n",
    "\n",
    "print(len(recipes_ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = []\n",
    "for ls in x:\n",
    "    #print(ls['ingredient_vector'])\n",
    "    s.append(ls['ingredient_vector'])\n",
    "s = np.array(s)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarities = cosine_similarity(s)\n",
    "similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
